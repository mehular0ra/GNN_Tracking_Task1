{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import tarfile\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.graphs = []\n",
    "\n",
    "        # Iterate through each .tar.gz file\n",
    "        for i in range(0, 10):\n",
    "            file_path = self.root_dir / f\"batch_1_{i}.tar.gz\"\n",
    "\n",
    "            # Extract all .pt files from the .tar.gz file\n",
    "            with tarfile.open(file_path, \"r:gz\") as tar:\n",
    "                for member in tar.getmembers():\n",
    "                    if member.name.endswith(\".pt\"):\n",
    "                        f = tar.extractfile(member)\n",
    "                        self.graphs.append(torch.load(f))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        graph = self.graphs[idx]\n",
    "        return graph\n",
    "\n",
    "    def display_graph_attributes(graph):\n",
    "        print(f\"Number of nodes: {graph.num_nodes}\")\n",
    "        print(f\"Number of edges: {graph.num_edges}\")\n",
    "        print(f\"Edge index: {graph.edge_index}\")\n",
    "        print(f\"Edge attributes: {graph.edge_attr}\")\n",
    "        print(f\"Node attributes: {graph.x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ExFileObject' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [193], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Dataset created from the tar files\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dataset \u001b[39m=\u001b[39m CustomDataset(\u001b[39m\"\u001b[39;49m\u001b[39mtar_data\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber of graphs: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(dataset)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn [192], line 22\u001b[0m, in \u001b[0;36mCustomDataset.__init__\u001b[0;34m(self, root_dir)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mif\u001b[39;00m member\u001b[39m.\u001b[39mname\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m.pt\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     21\u001b[0m     f \u001b[39m=\u001b[39m tar\u001b[39m.\u001b[39mextractfile(member)\n\u001b[0;32m---> 22\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgraphs\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39;49mload(f))\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/serialization.py:777\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    773\u001b[0m     \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    774\u001b[0m     \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    775\u001b[0m     \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    776\u001b[0m     orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n\u001b[0;32m--> 777\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_reader(opened_file) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    778\u001b[0m         \u001b[39mif\u001b[39;00m _is_torchscript_zip(opened_zipfile):\n\u001b[1;32m    779\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtorch.load\u001b[39m\u001b[39m'\u001b[39m\u001b[39m received a zip file that looks like a TorchScript archive\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    780\u001b[0m                           \u001b[39m\"\u001b[39m\u001b[39m dispatching to \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtorch.jit.load\u001b[39m\u001b[39m'\u001b[39m\u001b[39m (call \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtorch.jit.load\u001b[39m\u001b[39m'\u001b[39m\u001b[39m directly to\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    781\u001b[0m                           \u001b[39m\"\u001b[39m\u001b[39m silence this warning)\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mUserWarning\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/serialization.py:282\u001b[0m, in \u001b[0;36m_open_zipfile_reader.__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name_or_buffer) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 282\u001b[0m     \u001b[39msuper\u001b[39m(_open_zipfile_reader, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49mPyTorchFileReader(name_or_buffer))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ExFileObject' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "# Dataset created from the tar files\n",
    "dataset = CustomDataset(\"tar_data\")\n",
    "print(f\"Number of graphs: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph 1:\n",
      "Data(x=[419, 6], edge_index=[2, 4882], edge_attr=[4882, 4], y=[4882])\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the dataset and print the first graph\n",
    "for i in range(len(dataset)):\n",
    "    print(f\"Graph {i+1}:\")\n",
    "    print((dataset[i]))\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch.nn import Sequential as Seq, Linear, ReLU, Sigmoid\n",
    "\n",
    "\n",
    "class RelationalModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(RelationalModel, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, m):\n",
    "        return self.layers(m)\n",
    "\n",
    "\n",
    "class ObjectModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(ObjectModel, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, C):\n",
    "        return self.layers(C)\n",
    "\n",
    "\n",
    "class InteractionNetwork(MessagePassing):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(InteractionNetwork, self).__init__(aggr='add',\n",
    "                                                 flow='source_to_target')\n",
    "        self.R1 = RelationalModel(16, 4, hidden_size)\n",
    "        self.O = ObjectModel(10, 3, hidden_size)\n",
    "        self.R2 = RelationalModel(10, 1, hidden_size)\n",
    "        self.E: Tensor = Tensor()\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Tensor, edge_attr: Tensor) -> Tensor:\n",
    "\n",
    "        # propagate_type: (x: Tensor, edge_attr: Tensor)\n",
    "        x_tilde = self.propagate(\n",
    "            edge_index, x=x, edge_attr=edge_attr, size=None)\n",
    "\n",
    "        m2 = torch.cat([x_tilde[edge_index[1]],\n",
    "                        x_tilde[edge_index[0]],\n",
    "                        self.E], dim=1)\n",
    "        return torch.sigmoid(self.R2(m2))\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr):\n",
    "        # x_i --> incoming\n",
    "        # x_j --> outgoing\n",
    "        m1 = torch.cat([x_i, x_j, edge_attr], dim=1)\n",
    "        self.E = self.R1(m1)\n",
    "        return self.E\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        c = torch.cat([x, aggr_out], dim=1)\n",
    "        return self.O(c)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Shuffle dataset\n",
    "random.shuffle(dataset.graphs)\n",
    "\n",
    "# Split dataset into train and eval\n",
    "train_size = int(0.8 * len(dataset))\n",
    "eval_size = len(dataset) - train_size\n",
    "train_dataset, eval_dataset = random_split(dataset, [train_size, eval_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, eval_loader, criterion, optimizer, n_epochs):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # Train loop\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data in train_loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x, data.edge_index, data.edge_attr).squeeze(1)\n",
    "            # print(out.shape)\n",
    "            # print(data.y.shape)\n",
    "            # break\n",
    "            loss = criterion(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predicted = torch.round(torch.sigmoid(out))\n",
    "            correct += (predicted == data.y).sum().item()\n",
    "            total += data.y.size(0)\n",
    "\n",
    "            train_loss += loss.item()\n",
    "        # break\n",
    "        train_acc = 100 * correct / total\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Evaluation loop\n",
    "        eval_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in eval_loader:\n",
    "                data = data.to(device)\n",
    "                out = model(data.x, data.edge_index, data.edge_attr).squeeze(1)\n",
    "                loss = criterion(out, data.y)\n",
    "\n",
    "                # Calculate accuracy\n",
    "                predicted = torch.round(torch.sigmoid(out))\n",
    "                correct += (predicted == data.y).sum().item()\n",
    "                total += data.y.size(0)\n",
    "\n",
    "                eval_loss += loss.item()\n",
    "\n",
    "        eval_acc = 100 * correct / total\n",
    "        eval_loss /= len(eval_loader)\n",
    "\n",
    "        # Print epoch stats\n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Eval Loss: {eval_loss:.4f}, Eval Acc: {eval_acc:.2f}%\")\n",
    "\n",
    "    print(\"Training finished!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = InteractionNetwork(10).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss: 0.6598, Train Acc: 82.27%, Eval Loss: 0.6596, Eval Acc: 82.34%\n",
      "Epoch 1: Train Loss: 0.6581, Train Acc: 83.47%, Eval Loss: 0.6587, Eval Acc: 83.15%\n",
      "Epoch 2: Train Loss: 0.6576, Train Acc: 84.19%, Eval Loss: 0.6584, Eval Acc: 83.86%\n",
      "Epoch 3: Train Loss: 0.6573, Train Acc: 84.85%, Eval Loss: 0.6581, Eval Acc: 84.55%\n",
      "Epoch 4: Train Loss: 0.6571, Train Acc: 85.42%, Eval Loss: 0.6578, Eval Acc: 85.29%\n",
      "Epoch 5: Train Loss: 0.6569, Train Acc: 86.07%, Eval Loss: 0.6576, Eval Acc: 86.00%\n",
      "Epoch 6: Train Loss: 0.6567, Train Acc: 86.64%, Eval Loss: 0.6574, Eval Acc: 86.53%\n",
      "Epoch 7: Train Loss: 0.6565, Train Acc: 87.06%, Eval Loss: 0.6573, Eval Acc: 86.85%\n",
      "Epoch 8: Train Loss: 0.6564, Train Acc: 87.38%, Eval Loss: 0.6571, Eval Acc: 87.17%\n",
      "Epoch 9: Train Loss: 0.6562, Train Acc: 87.66%, Eval Loss: 0.6570, Eval Acc: 87.44%\n",
      "Epoch 10: Train Loss: 0.6561, Train Acc: 87.92%, Eval Loss: 0.6569, Eval Acc: 87.68%\n",
      "Epoch 11: Train Loss: 0.6560, Train Acc: 88.14%, Eval Loss: 0.6568, Eval Acc: 87.81%\n",
      "Epoch 12: Train Loss: 0.6559, Train Acc: 88.30%, Eval Loss: 0.6567, Eval Acc: 87.93%\n",
      "Epoch 13: Train Loss: 0.6558, Train Acc: 88.44%, Eval Loss: 0.6566, Eval Acc: 88.09%\n",
      "Epoch 14: Train Loss: 0.6557, Train Acc: 88.58%, Eval Loss: 0.6565, Eval Acc: 88.23%\n",
      "Epoch 15: Train Loss: 0.6556, Train Acc: 88.70%, Eval Loss: 0.6564, Eval Acc: 88.34%\n",
      "Epoch 16: Train Loss: 0.6556, Train Acc: 88.80%, Eval Loss: 0.6563, Eval Acc: 88.44%\n",
      "Epoch 17: Train Loss: 0.6555, Train Acc: 88.90%, Eval Loss: 0.6563, Eval Acc: 88.54%\n",
      "Epoch 18: Train Loss: 0.6554, Train Acc: 89.00%, Eval Loss: 0.6562, Eval Acc: 88.66%\n",
      "Epoch 19: Train Loss: 0.6554, Train Acc: 89.11%, Eval Loss: 0.6561, Eval Acc: 88.74%\n",
      "Epoch 20: Train Loss: 0.6553, Train Acc: 89.21%, Eval Loss: 0.6561, Eval Acc: 88.82%\n",
      "Epoch 21: Train Loss: 0.6552, Train Acc: 89.32%, Eval Loss: 0.6560, Eval Acc: 88.93%\n",
      "Epoch 22: Train Loss: 0.6552, Train Acc: 89.40%, Eval Loss: 0.6559, Eval Acc: 89.02%\n",
      "Epoch 23: Train Loss: 0.6551, Train Acc: 89.47%, Eval Loss: 0.6559, Eval Acc: 89.10%\n",
      "Epoch 24: Train Loss: 0.6551, Train Acc: 89.52%, Eval Loss: 0.6558, Eval Acc: 89.19%\n",
      "Epoch 25: Train Loss: 0.6550, Train Acc: 89.58%, Eval Loss: 0.6558, Eval Acc: 89.26%\n",
      "Epoch 26: Train Loss: 0.6550, Train Acc: 89.64%, Eval Loss: 0.6557, Eval Acc: 89.31%\n",
      "Epoch 27: Train Loss: 0.6549, Train Acc: 89.70%, Eval Loss: 0.6556, Eval Acc: 89.36%\n",
      "Epoch 28: Train Loss: 0.6549, Train Acc: 89.75%, Eval Loss: 0.6556, Eval Acc: 89.39%\n",
      "Epoch 29: Train Loss: 0.6548, Train Acc: 89.79%, Eval Loss: 0.6555, Eval Acc: 89.42%\n",
      "Epoch 30: Train Loss: 0.6547, Train Acc: 89.82%, Eval Loss: 0.6555, Eval Acc: 89.46%\n",
      "Epoch 31: Train Loss: 0.6547, Train Acc: 89.87%, Eval Loss: 0.6554, Eval Acc: 89.53%\n",
      "Epoch 32: Train Loss: 0.6546, Train Acc: 89.92%, Eval Loss: 0.6554, Eval Acc: 89.58%\n",
      "Epoch 33: Train Loss: 0.6546, Train Acc: 89.97%, Eval Loss: 0.6553, Eval Acc: 89.62%\n",
      "Epoch 34: Train Loss: 0.6545, Train Acc: 90.02%, Eval Loss: 0.6553, Eval Acc: 89.67%\n",
      "Epoch 35: Train Loss: 0.6545, Train Acc: 90.06%, Eval Loss: 0.6552, Eval Acc: 89.71%\n",
      "Epoch 36: Train Loss: 0.6544, Train Acc: 90.11%, Eval Loss: 0.6552, Eval Acc: 89.76%\n",
      "Epoch 37: Train Loss: 0.6544, Train Acc: 90.16%, Eval Loss: 0.6551, Eval Acc: 89.80%\n",
      "Epoch 38: Train Loss: 0.6543, Train Acc: 90.22%, Eval Loss: 0.6551, Eval Acc: 89.86%\n",
      "Epoch 39: Train Loss: 0.6543, Train Acc: 90.28%, Eval Loss: 0.6550, Eval Acc: 89.92%\n",
      "Epoch 40: Train Loss: 0.6542, Train Acc: 90.34%, Eval Loss: 0.6550, Eval Acc: 90.01%\n",
      "Epoch 41: Train Loss: 0.6542, Train Acc: 90.39%, Eval Loss: 0.6549, Eval Acc: 90.09%\n",
      "Epoch 42: Train Loss: 0.6541, Train Acc: 90.45%, Eval Loss: 0.6548, Eval Acc: 90.18%\n",
      "Epoch 43: Train Loss: 0.6541, Train Acc: 90.51%, Eval Loss: 0.6548, Eval Acc: 90.26%\n",
      "Epoch 44: Train Loss: 0.6540, Train Acc: 90.56%, Eval Loss: 0.6547, Eval Acc: 90.35%\n",
      "Epoch 45: Train Loss: 0.6540, Train Acc: 90.63%, Eval Loss: 0.6547, Eval Acc: 90.45%\n",
      "Epoch 46: Train Loss: 0.6540, Train Acc: 90.70%, Eval Loss: 0.6546, Eval Acc: 90.54%\n",
      "Epoch 47: Train Loss: 0.6539, Train Acc: 90.77%, Eval Loss: 0.6546, Eval Acc: 90.60%\n",
      "Epoch 48: Train Loss: 0.6539, Train Acc: 90.83%, Eval Loss: 0.6545, Eval Acc: 90.68%\n",
      "Epoch 49: Train Loss: 0.6538, Train Acc: 90.89%, Eval Loss: 0.6545, Eval Acc: 90.74%\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "# Train with binary cross-entropy loss\n",
    "bce_loss = torch.nn.BCEWithLogitsLoss()\n",
    "train_model(model, train_loader, eval_loader, bce_loss, optimizer, 50)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Imbalance Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of label 0 samples: 3182276\n",
      "Number of label 1 samples: 697606\n",
      "Ratio of label 0 samples to label 1 samples: 4.561709618323237\n",
      "Imbalance ratio: 0.17980082899428385\n"
     ]
    }
   ],
   "source": [
    "label_0_count = sum([dataset[i].y.eq(0).sum().item()\n",
    "                    for i in range(len(dataset))])\n",
    "label_1_count = sum([dataset[i].y.eq(1).sum().item()\n",
    "                    for i in range(len(dataset))])\n",
    "\n",
    "print(\"Number of label 0 samples:\", label_0_count)\n",
    "print(\"Number of label 1 samples:\", label_1_count)\n",
    "print(\"Ratio of label 0 samples to label 1 samples:\",\n",
    "      label_0_count / label_1_count)\n",
    "print(\"Imbalance ratio:\", label_1_count / (label_0_count + label_1_count))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class imbalance is a comman problem is ML which can be make model biased. There are several approaches to deal with it: \n",
    "1. Resampling data: undersample the majority class by removing some of the samples\n",
    "2. Class weighting: assign higher weights to the minority class and lower weights to the majority class during training\n",
    "3. Data augmentation: new samples by applying transformations to the existing samples\n",
    "4. Ensemble methods: bagging or boosting to train multiple models on different samples of the data and combine their predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using class weighting during training where the minority class is assigned a higher weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss: 0.9433, Train Acc: 88.52%, Eval Loss: 0.9401, Eval Acc: 85.66%\n",
      "Epoch 1: Train Loss: 0.9392, Train Acc: 84.73%, Eval Loss: 0.9353, Eval Acc: 82.52%\n",
      "Epoch 2: Train Loss: 0.9356, Train Acc: 83.66%, Eval Loss: 0.9336, Eval Acc: 83.32%\n",
      "Epoch 3: Train Loss: 0.9342, Train Acc: 84.06%, Eval Loss: 0.9326, Eval Acc: 83.34%\n",
      "Epoch 4: Train Loss: 0.9332, Train Acc: 83.59%, Eval Loss: 0.9317, Eval Acc: 82.20%\n",
      "Epoch 5: Train Loss: 0.9319, Train Acc: 83.55%, Eval Loss: 0.9303, Eval Acc: 83.34%\n",
      "Epoch 6: Train Loss: 0.9310, Train Acc: 83.91%, Eval Loss: 0.9297, Eval Acc: 83.49%\n",
      "Epoch 7: Train Loss: 0.9304, Train Acc: 84.16%, Eval Loss: 0.9291, Eval Acc: 83.82%\n",
      "Epoch 8: Train Loss: 0.9298, Train Acc: 84.20%, Eval Loss: 0.9288, Eval Acc: 83.93%\n",
      "Epoch 9: Train Loss: 0.9292, Train Acc: 84.29%, Eval Loss: 0.9284, Eval Acc: 84.21%\n",
      "Epoch 10: Train Loss: 0.9288, Train Acc: 84.41%, Eval Loss: 0.9281, Eval Acc: 84.56%\n",
      "Epoch 11: Train Loss: 0.9284, Train Acc: 84.48%, Eval Loss: 0.9278, Eval Acc: 84.53%\n",
      "Epoch 12: Train Loss: 0.9280, Train Acc: 84.51%, Eval Loss: 0.9271, Eval Acc: 84.04%\n",
      "Epoch 13: Train Loss: 0.9272, Train Acc: 84.27%, Eval Loss: 0.9269, Eval Acc: 82.95%\n",
      "Epoch 14: Train Loss: 0.9261, Train Acc: 83.84%, Eval Loss: 0.9260, Eval Acc: 83.17%\n",
      "Epoch 15: Train Loss: 0.9255, Train Acc: 84.01%, Eval Loss: 0.9256, Eval Acc: 83.39%\n",
      "Epoch 16: Train Loss: 0.9249, Train Acc: 84.19%, Eval Loss: 0.9249, Eval Acc: 83.84%\n",
      "Epoch 17: Train Loss: 0.9247, Train Acc: 84.24%, Eval Loss: 0.9247, Eval Acc: 83.69%\n",
      "Epoch 18: Train Loss: 0.9244, Train Acc: 84.30%, Eval Loss: 0.9244, Eval Acc: 83.79%\n",
      "Epoch 19: Train Loss: 0.9241, Train Acc: 84.33%, Eval Loss: 0.9242, Eval Acc: 83.89%\n",
      "Epoch 20: Train Loss: 0.9239, Train Acc: 84.41%, Eval Loss: 0.9241, Eval Acc: 83.96%\n",
      "Epoch 21: Train Loss: 0.9237, Train Acc: 84.45%, Eval Loss: 0.9240, Eval Acc: 83.97%\n",
      "Epoch 22: Train Loss: 0.9235, Train Acc: 84.50%, Eval Loss: 0.9239, Eval Acc: 84.13%\n",
      "Epoch 23: Train Loss: 0.9232, Train Acc: 84.58%, Eval Loss: 0.9237, Eval Acc: 84.25%\n",
      "Epoch 24: Train Loss: 0.9231, Train Acc: 84.63%, Eval Loss: 0.9236, Eval Acc: 84.44%\n",
      "Epoch 25: Train Loss: 0.9229, Train Acc: 84.71%, Eval Loss: 0.9235, Eval Acc: 84.52%\n",
      "Epoch 26: Train Loss: 0.9227, Train Acc: 84.76%, Eval Loss: 0.9234, Eval Acc: 84.52%\n",
      "Epoch 27: Train Loss: 0.9226, Train Acc: 84.77%, Eval Loss: 0.9233, Eval Acc: 84.51%\n",
      "Epoch 28: Train Loss: 0.9224, Train Acc: 84.77%, Eval Loss: 0.9231, Eval Acc: 84.52%\n",
      "Epoch 29: Train Loss: 0.9223, Train Acc: 84.79%, Eval Loss: 0.9230, Eval Acc: 84.52%\n",
      "Epoch 30: Train Loss: 0.9222, Train Acc: 84.80%, Eval Loss: 0.9230, Eval Acc: 84.50%\n",
      "Epoch 31: Train Loss: 0.9221, Train Acc: 84.83%, Eval Loss: 0.9229, Eval Acc: 84.52%\n",
      "Epoch 32: Train Loss: 0.9220, Train Acc: 84.86%, Eval Loss: 0.9228, Eval Acc: 84.53%\n",
      "Epoch 33: Train Loss: 0.9219, Train Acc: 84.90%, Eval Loss: 0.9227, Eval Acc: 84.56%\n",
      "Epoch 34: Train Loss: 0.9218, Train Acc: 84.94%, Eval Loss: 0.9226, Eval Acc: 84.63%\n",
      "Epoch 35: Train Loss: 0.9217, Train Acc: 84.99%, Eval Loss: 0.9225, Eval Acc: 84.71%\n",
      "Epoch 36: Train Loss: 0.9216, Train Acc: 85.03%, Eval Loss: 0.9225, Eval Acc: 84.80%\n",
      "Epoch 37: Train Loss: 0.9215, Train Acc: 85.08%, Eval Loss: 0.9224, Eval Acc: 84.84%\n",
      "Epoch 38: Train Loss: 0.9214, Train Acc: 85.11%, Eval Loss: 0.9223, Eval Acc: 84.88%\n",
      "Epoch 39: Train Loss: 0.9213, Train Acc: 85.16%, Eval Loss: 0.9222, Eval Acc: 84.86%\n",
      "Epoch 40: Train Loss: 0.9212, Train Acc: 85.21%, Eval Loss: 0.9220, Eval Acc: 84.87%\n",
      "Epoch 41: Train Loss: 0.9211, Train Acc: 85.24%, Eval Loss: 0.9219, Eval Acc: 84.87%\n",
      "Epoch 42: Train Loss: 0.9210, Train Acc: 85.21%, Eval Loss: 0.9218, Eval Acc: 84.79%\n",
      "Epoch 43: Train Loss: 0.9209, Train Acc: 85.05%, Eval Loss: 0.9216, Eval Acc: 84.65%\n",
      "Epoch 44: Train Loss: 0.9207, Train Acc: 84.95%, Eval Loss: 0.9214, Eval Acc: 84.57%\n",
      "Epoch 45: Train Loss: 0.9206, Train Acc: 85.00%, Eval Loss: 0.9215, Eval Acc: 84.55%\n",
      "Epoch 46: Train Loss: 0.9205, Train Acc: 85.01%, Eval Loss: 0.9214, Eval Acc: 84.54%\n",
      "Epoch 47: Train Loss: 0.9203, Train Acc: 85.03%, Eval Loss: 0.9212, Eval Acc: 84.52%\n",
      "Epoch 48: Train Loss: 0.9203, Train Acc: 85.06%, Eval Loss: 0.9212, Eval Acc: 84.52%\n",
      "Epoch 49: Train Loss: 0.9202, Train Acc: 85.08%, Eval Loss: 0.9212, Eval Acc: 84.46%\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "# Train with class-balanced binary cross-entropy loss\n",
    "n_positives =  sum([dataset[i].y.eq(1).sum().item() for i in range(len(dataset))])\n",
    "n_negatives = sum([dataset[i].y.eq(0).sum().item() for i in range(len(dataset))])\n",
    "pos_weight = torch.tensor(n_negatives / n_positives, dtype=torch.float32)\n",
    "bce_loss_class_balanced = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "# bce_loss_class_balanced = torch.nn.BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "train_model(model, train_loader, eval_loader, bce_loss_class_balanced, optimizer, 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "872136613a9aef1dec7b951767fb3c7d3b62cc0515569975c246928404dfe64b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
